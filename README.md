# parallel-inference-generator
Parallel Inference Generator feeds prompts to your hosted llm endpoints and outputs an excel sheet with the required metrics from the endpoint. It is assumed that the hosted endpoint uses `tgi`.
